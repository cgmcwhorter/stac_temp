\documentclass[11pt,letterpaper]{article}
\usepackage[lmargin=1in,rmargin=1in,bmargin=1in,tmargin=1in]{geometry}
\usepackage{quiz}


% -------------------
% Content
% -------------------
\begin{document}
\thispagestyle{title}

% Quiz 1
\quizsol \textit{True/False}: There must be a solution to the equation $x \left( e^{2x} - 2 \right)= 15$.  \pspace

\sol The statement is \textit{true}. The equation $x \left( e^{2x} - 2 \right)= 15$ has a solution if and only if the equation $x \left( e^{2x} - 2 \right) - 15= 0$ has a solution. But the equation $x \left( e^{2x} - 2 \right) - 15= 0$ has a solution if and only if the function $f(x)= x \left( e^{2x} - 2 \right) - 15$ has a root. The function $f(x)$ is a continuous function---being the composition, sum/difference, and product of continuous functions. We know that $f(0)= 0 (1 - 2) - 15= -15 < 0$. Furthermore, there are clearly values for which $f(x)$ is positive because $\ds \lim_{x \to \infty} f(x)= \infty$. But then by the Intermediate Value Theorem, there must be a value $x_0 \in [-15, \infty)$ such that $f(x_0)= 0$. This proves that $f(x)$ has a root; equivalently, that $x \left( e^{2x} - 2 \right)= 15$ has a solution. \pvspace{1.3cm}



% Quiz 2
\quizsol \textit{True/False}: The linearization of a function $f(x)$ (if it exists) is an example of a truncated Taylor series for $f(x)$. \pspace

\sol The statement is \textit{true}. The linearization of a function at $x= a$ (if it exists) is the tangent line at $x= a$. If $f(x)$ is differentiable on an open interval containing $x= a$, we know the tangent line is $\ell(x)= f(a) + f'(a) \big(x - a \big)$. If $f(x)$ is smooth on an open interval containing $x= a$, its Taylor series is given by\dots
	\[
	\sum_{k=0}^\infty \dfrac{f^{(k)}(a)}{k!} \, (x - a)^k= \dfrac{f^{(0)}(a)}{0!} \, (x - a)^0 + \dfrac{f^{(1)}(a)}{1!} \, (x - a)^1 + \dfrac{f^{(2)}(a)}{2!} \, (x - a)^2 + \cdots
	\]
Truncating this series at the first derivative, this is\dots
	\[
	\dfrac{f^{(0)}(a)}{0!} \, (x - a)^0 + \dfrac{f^{(1)}(a)}{1!} \, (x - a)^1= \dfrac{f(a)}{1} \cdot 1 + \dfrac{f'(a)}{1} \, (x - a)= f(a) + f'(a) \big(x - a \big)
	\]
But this is precisely the tangent line of $f(x)$ at $x= a$. \pvspace{1.3cm}



% Quiz 3
\quizsol \textit{True/False}: Suppose $f(x)$ has a Taylor series which converges to $f$ on $(-1, 3]$ and converges to $f$ at $x= -5$. Then the center was $x_0= 1$ and there must be at least one other value outside $(-1, 3]$ for which the Taylor series converges. \pspace

\sol The statement is \textit{true}. Because the Taylor series converges to $f(x)$ on the interval $(-1, 3]$, the center of the Taylor series must be $x_0= \frac{3 + (-1)}{2}= \frac{2}{2}= 1$. Because the Taylor series converges to $f(x)$ on $(-1, 3]$, the radius of convergence is at least $\frac{3 - (-1)}{2}= \frac{4}{2}= 2$. We know that the Taylor series converges for all values less than the radius of convergence from its center and diverges for values greater than its radius of convergence from the center. What happens at a distance equal to the radius of convergence from the center varies. Because the Taylor series converges at $x= -5$, the radius of convergence must then be at least $1 - (-5)= 6$. But then the Taylor series converges for at least all the values in $(-5, 7)$. In particular, there is at least one other value outside $(-1, 3]$ for which the Taylor series converges. \pvspace{1.3cm}





\newpage





% Quiz 4
\quizsol \textit{True/False}: The Intermediate Value Theorem and Mean Value Theorem apply to the function $f(x)= |x|$ on $[-1, 1]$. \pspace

\sol The statement is \textit{false}. Recall that the Intermediate Value Theorem states that if $f(x)$ is continuous on $[a, b]$ and $c$ is a value between $f(a)$ and $f(b)$, there exists $x_0 \in [a, b]$ such that $f(x_0)= c$. The Mean Value Theorem states that if $f(x)$ is continuous on $[a, b]$ and differentiable on $(a, b)$, there exists $c \in (a, b)$ such that $f(b) - f(a)= f'(c) \big(b - a \big)$. The function $f(x)= |x|$ is given by\dots
	\[
	|x|= 
	\begin{cases}
	x, & x \geq 0 \\
	-x, & x < 0
	\end{cases}
	\]
Clearly, $x$ and $-x$ are everywhere continuous (they are polynomials) and $\ds \lim_{x \to 0^-} f(x)= \lim_{x \to 0^-} -x= 0$, $\lim_{x \to 0^+} f(x)= \lim_{x \to 0^+} x= 0$, and $f(0)= 0$, the function $f(x)= |x|$ is continuous on $[-1, 1]$. Therefore, the Intermediate Value Theorem applies to $f(x)$ for any value in $\text{range}_{[-1,1]} f(x)$, i.e. on the interval $[0, 1]$. While the function $f(x)= |x|$ is continuous on $[-1, 1]$, it is not differentiable on $(-1, 1)$. We know that $|x|$ is differentiable on $(-1, 1) \setminus \{ 0 \}$, i.e. $|x|$ is not differentiable at $x= 0$. To see that the theorem fails, consider the case with $a= -1$ and $b= 1$. Then $f(b) - f(a)= f(1) - f(-1)= 1 - 1= 0$. We have $f'(c) \big(b - a \big)= f'(x_0) \big(1 - (-1) \big)= 2f'(c)$. Then we must have $f'(c)= 0$. But where $f'(x)$ exists, we either have $f'(x)= 1$ or $f'(x)= -1$. Therefore, it is impossible that $f'(c)= 0$. \pvspace{1.3cm}



% Quiz 5
\quizsol \textit{True/False}: IEEE floating point numbers are a special type of binary representation for real numbers. While these numbers cannot represent all real numbers for a fixed bit length, they represent all real numbers in their range and have the same arithmetic operations as the real numbers they represent. \pspace

\sol The statement is \textit{false}. Because only a finite number of bits are used to track the sign, exponent, and significand, the system can only store finitely many discrete values. In particular, fixing a number of bits, even in the interval containing all the representable floating point numbers, $[r_{\text{min}}, r_{\text{max}}]$, not every real number in this interval is representable. For floating point numbers, we can perform all the same basic arithmetic operations, i.e. addition, subtraction, multiplication, division, etc. However, these operations do not have the same properties as their real number counterparts due to rounding/truncation. For instance, addition of floating point numbers need not be associative: if $x, y, z \in \mathbb{F}$, we need not have\dots
	\[
	x + (y + z)= (x + y) + z
	\] \pvspace{1.3cm}



% Quiz 6
\quizsol \textit{True/False}: The function $f(x)= \sqrt{1 - x^2} - 1$ has issues with conditioning and catastrophic cancellation for values $x \approx 0$. \pspace

\sol The statement is \textit{false}. First, observe that for $x \approx 0$, we know that $x^2 \approx 0$. But then for $x \approx 0$, $1 - x^2 \approx 1 - 0= 1$. But then for $x \approx 0$, we know that $\sqrt{1 - x^2} \approx \sqrt{1}= 1$. [This requires a bit more justification, but one can easily make use of the differentiability or continuity of the square root function to justify this.] But then as written, one would be subtracting to `near' quantities for $x \approx 0$, which could result in catastrophic cancellation. To prevent this, we should rewrite the function $f(x)$ to avoid this:
	\[
	f(x)= \sqrt{1 - x^2} - 1= \big( \sqrt{1 - x^2} - 1 \big) \cdot \dfrac{\sqrt{1 - x^2} + 1}{\sqrt{1 - x^2} + 1}= \dfrac{-x^2}{\sqrt{1 - x^2} + 1}
	\]
We know the condition number to evaluate a (differentiable) function $f(x)$ at $x= x_0$ is approximately $\frac{x f'(x)}{f(x)}$. Here, we have $f(x)= \sqrt{1 - x^2} - 1$ and $f'(x)= -\frac{x}{\sqrt{1 - x^2}}$. But then\dots
	\[
	\dfrac{x f'(x)}{f(x)}= \dfrac{x \cdot -\frac{x}{\sqrt{1 - x^2}}}{\sqrt{1 - x^2} - 1}= \dfrac{\frac{-x^2}{\sqrt{1 - x^2}}}{\sqrt{1 - x^2} - 1}= \dfrac{-x^2}{(1 - x^2) - \sqrt{1 - x^2}}= \dfrac{x^2}{x^2 - 1 + \sqrt{1 - x^2}}
	\]
But we have\dots
	\[
	\lim_{x \to 0} \dfrac{x f'(x)}{f(x)}= \lim_{x \to 0} \dfrac{x^2}{x^2 - 1 + \sqrt{1 - x^2}}= 2
	\]
We can consider this condition number not `too large'---certainly, $\kappa < \infty$. Therefore, we would like consider the evaluation of $f(x)$ `near zero' as not having issues with conditioning. Therefore, while the quiz statement was correct on the issue of catastrophic cancellation, it was wrong about the issue of conditioning. \pvspace{1.3cm}



% Quiz 7
\quizsol \textit{True/False}: If $f(x)$ has a root on the interval $[a, b]$, then the bisection method will find this approximate this root to any desired accuracy. Furthermore, the actual error in the approximation is independent of the function. \pspace

\sol The statement is \textit{false}. Simply because $f(x)$ has a root on $[a, b]$ \textit{does not} imply that the bisection method applies to this interval. The bisection method requires the intermediate value theorem. Therefore, if $f(a)$ and $f(b)$ do not have opposite signs, the bisection method cannot proceed. For instance, consider the function $f(x)= (x - 2)^2 - 1= x^2 - 4x + 3$ clearly has roots at $x= 1$ and $x= 3$. In particular, $f(x)$ has roots on the interval $[0, 4]$. However, $f(0)= 3 > 0$ and $f(4)= 3 > 0$. Therefore, the intermediate value theorem and bisection method do not apply to $f(x)$ on the interval $[0, 4]$. Furthermore, if $r$ is a root of $f(x)$ on the interval $[a, b]$, assuming that the bisection method applies to $f(x)$ on $[a, b]$, we cannot control the \textit{actual} error but the \textit{maximal} error. For the bisection method, we know that after $n$ iterations, the maximal error is $\frac{b_0 - a_0}{2^{n + 1}}$; that is, if our approximation to $r$ after $n$ iterations of the bisection method is $x^*$, then $|x^* - r| < \frac{b_0 - a_0}{2^{n + 1}}$. The statement of the quiz is then wrong in two aspects. First, the bisection will not necessarily find a root for $f(x)$ on any interval that contains a root for $f(x)$. Second, while the actual error will depend on the function, it is the maximal error, $\frac{b_0 - a_0}{2^{n + 1}}$, that we can control and is independent of the function. \pvspace{1.3cm}



% Quiz 8
\quizsol \textit{True/False}: If $f(x)$ has a root on $[a, b]$ for which the bisection method can be used, then the bisection method will converge to the root linearly while Newton's method will converge to the root quadratically. \pspace

\sol The statement it \textit{false}. Suppose that $f(x)$ has a zero on $[a, b]$ and that the bisection method applies to $f(x)$ on $[a, b]$, i.e. $f(a)$ and $f(b)$ have opposite signs. We know that the bisection method will always produce approximations $x_n^*$ such that $x_n^* \to r$, where $r$ is some root of $f(x)$ on $[a, b]$. Furthermore, we know that the rate of this convergence is linear. However, the nature and rate of convergence for Newton's method depends on the nature of the function and starting position. For instance, Newton's method may not converge for some initial starting position. Furthermore, the rate of convergence for Newton's method need not always be quadratic. In the case of root with multiplicity greater than one, Newton's method converges linearly. To see that the bisection method will converge when Newton's method may not, consider the case of $f(x)= \sqrt[3]{x}$ on $[-1, 1]$. Clearly, the only root of $f(x)$ on $[-1, 1]$ is $x= 0$. We know that $f(-1)= -1 < 0$ and $f(1)= 1 > 0$ so that the bisection method applies to $f(x)$ on $[-1, 1]$. However, we have\dots
	\[
	x_k - \dfrac{f(x_k)}{f'(x_k)}= x - \dfrac{\sqrt[3]{x_k}}{\frac{1}{3} \frac{1}{\sqrt[3]{x^2}}}= x_k - 3x_k= -2x_k
	\]
But then beginning Newton's Method at $x_0= a \in [-1, 1] \setminus \{ 0 \}$, we can see that $x_k= (-2)^k a$. But then Newton's Method will diverge for any $x_0 \neq 0$. It may be possible that the bisection and Newton's method both converge linearly to the root. For instance, consider the function $g(x)= x^3$ on $[-1, 1]$. Clearly, the only root of $g(x)$ on $[-1, 1]$ is $x= 0$ and this root has multiplicity three. We know that $g(-1)= -1 < 0$ and $g(1)= 1 > 0$. Therefore, the bisection method applies to $g(x)$ on $[-1, 1]$ so that the bisection method converges linearly to zero on $[-1, 1]$. It is not very difficult to prove that Newton's method will converge for any $a \neq 0$. However, because the root has multiplicity greater than one, the rate of this convergence is only linear. \pvspace{1.3cm}



% Quiz 9
\quizsol \textit{True/False}: Using two initial starting values, $x_0$ and $x_1$, the secant method is given by\dots
	\[
	x_{n+1} = \dfrac{f(x_n) (x_n - x_{n-1})}{f(x_n) - f(x_{n-1})}
	\]
Moreover, the secant method is derived from Newton's method by replacing $f'(x_n)$ with the approximation $f'(x_n) \approx \frac{f(x_n) - f(x_{n-1})}{x_n - x_{n-1}}$. \pspace

\sol The statement is \textit{true}. One disadvantage of Newton's method is that it (technically) requires that the function is differentiable---or at least at any $x_n$ involved in the iterations. This requires constant evaluations of $f'(x)$---which may be `expensive.' If $r$ is a solution to the equation $f(x) = 0$, i.e. a root of $f(x)$, then it should be that $f'(x_n) \approx \frac{f(x_n) - f(x_{n-1})}{x_n - x_{n-1}}$, if $x_{n-1}, x_n \approx r$. Using this fact in Newton's method, we have\dots
	\[
	x_{n+1} = x_n - \dfrac{f(x_n)}{f'(x_n)} \approx x_n - \dfrac{f(x_n)}{\frac{f(x_n) - f(x_{n-1})}{x_n - x_{n-1}}}= x_n - \dfrac{f(x_n) (x_n - x_{n-1})}{f(x_n) - f(x_{n-1})}
	\]
One should not combine the terms on the right to avoid catastrophic cancellation. When the method converges, it converges superlinearly---but not quadratic convergence as in Newton's method. The advantage of the secant method is that it does not require differentiability or evaluations of derivatives. \pvspace{1.3cm}



% Quiz 10
\quizsol \textit{True/False}: Let $p(x)$ be a real polynomial of degree $n$. If one iteratively approximates the $n$ roots of $p(x)$, say $\widehat{r}_1, \widehat{r}_2, \ldots, \widehat{r}_n$, using a root finding method and deflation, then $p(x) \approx a_n (x - r_1) (x - r_2) \cdots (x - r_n)$, where $a_n$ is the leading coefficient of $p(x)$. \pspace

\sol The statement is \textit{false}. At each stage, one only approximates an approximation of a root for $p(x)$, say $\widehat{r}_i$. But then after deflation (division of $p(x)$ by $x - r_i$), one obtains a polynomial $\widehat{p}_i(x)$ such that $p(x) \approx (x - \widehat{r}_i) \widehat{p}_i(x)$. At each stage, the error in approximating $r_i$ `propagates' to error in $\widehat{p}_i(x)$. For instance, if $r_j$ is an \textit{exact} root of $p(x)$ and $\tilde{r}_j$ is the approximate root, we have $p(x) = (x - r_j) p_j(x)$ and $p(x) \approx (x - \tilde{r}_j) \widehat{p}_j(x)$. But it need not be that $p_j(x) \approx \widehat{p}_j(x)$. Indeed, the types of roots (rational, real, complex) for $p_j(x)$ need not be the same as $\widehat{p}_j(x)$. Especially for large $n$, this can result in a product of linear terms that does not approximate the polynomial $p(x)$ nor its values. One need take care at each stage of the evaluation and track errors `locally' and `globally', i.e. for the roots and the collection of coefficients, to be sure that the resulting product of linear terms consists of true approximate roots and replicates the values of $p(x)$. \pvspace{1.3cm}



% Quiz 11
\quizsol \textit{True/False}: The method of linear regression can be applied to find the best fit model of the form $y= Ae^{ct}$ for a given dataset. \pspace

\sol The statement is \textit{true}. So long as all the $y$-values in the dataset are positive, we have\dots
	\[
	\begin{gathered}
	y= Ae^{ct} \\[0.3cm]
	\ln y = \ln \left( A e^{ct} \right) \\[0.3cm]
	\ln y = \ln \left( e^{ct} \right) + \ln(A) \\[0.3cm]
	\ln y = ct + \ln(A)
	\end{gathered}
	\]
Defining $Y= \ln y$, $b_1= c$, $X= t$, and $b_0= \ln(A)$, this equation is $Y= b_1 X + b_0$. But if the original data was (approximately) exponential, the transformed data is (approximately) linear. We can then use linear regression to approximate $b_1$ and $b_0$. But then $A= e^{b_0}$ and $c= b_1$ in the original model so that one can use the approximations to $b_0, b_1$ to find approximations to the best $A, c$ in the original model. \pvspace{1.3cm}



% Quiz 12
\quizsol \textit{True/False}: If gradient descent converges, it converges to the global minimum. \pspace

\sol The statement is \textit{false}. If gradient descent converges to the location of a minimum, the corresponding minimum need not be a global minimum but rather a local minimum. Recall that gradient descent begins with an initial value $\mathbf{x}_0$ and then creates subsequent approximations to the location of a minimum via $\mathbf{x}_{n+1}= \mathbf{x}_n - \alpha \nabla f(\mathbf{x}_n)$, where $\alpha$ is some scaling factor (or learning rate). For instance, consider the function $f(x)= x^4 + x^3 - 2x^2$. One can find the minima exactly, which occur at $m_1= \frac{-3 - \sqrt{73}}{8} \approx -1.443$ (where the output is $\approx -2.83342$) and $m_2= \frac{-3 + \sqrt{73}}{8} \approx 0.693$ (where the value is $\approx -0.397046$). Using $x_0= 1$ and $\alpha= 0.01$, $x_n \to m_2$, despite the output at $m_1$ being smaller. Worse yet, if gradient descent converges, it need not even converge to a minimum at all! It is simple to see that $f(x)= x^3$ has no minimum. But applying gradient descent with $x_0= 1/2$ and $\alpha= 0.01$, it is easy to see that $x_n \to 0$, despite the fact that $x= 0$ is a saddle point for $f(x)= x^3$. 



%Suppose you have a collection of data points $\{ (x_i, y_i) \}_{i=1}^n$, where the $x_i$ are all distinct. Suppose $\mathcal{L}(x)$ and $N(x)$ are the Lagrange and Newton polynomials interpolating this collection of data. It must be that $\mathcal{L}(x) \equiv N(x)$.






Let $f(x)$ be a continuous functions. Let $\{ (x_i, y_i) \}_{i=1}^n$ be a collection of distinct points on the graph of $f(x)$. The larger the number of points used, the better the approximation for $f(x)$ the interpolating polynomial through these points will be. 


























\end{document}